{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 6s 0us/step\n",
      "WARNING:tensorflow:From /home/anonymous/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              (None, 49)                38465     \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 28, 28, 1)         39200     \n",
      "=================================================================\n",
      "Total params: 77,665\n",
      "Trainable params: 77,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/anonymous/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.2576 - val_loss: 0.1677\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.1510 - val_loss: 0.1357\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 0.1272 - val_loss: 0.1175\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.1123 - val_loss: 0.1054\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.1023 - val_loss: 0.0973\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0953 - val_loss: 0.0915\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0902 - val_loss: 0.0871\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0867 - val_loss: 0.0843\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0843 - val_loss: 0.0824\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0828 - val_loss: 0.0813\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.0818 - val_loss: 0.0804\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0812 - val_loss: 0.0801\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0807 - val_loss: 0.0797\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.0804 - val_loss: 0.0793\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.0801 - val_loss: 0.0791\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0799 - val_loss: 0.0789\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.0798 - val_loss: 0.0788\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0796 - val_loss: 0.0787\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0795 - val_loss: 0.0786\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0794 - val_loss: 0.0785\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0793 - val_loss: 0.0784\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.0792 - val_loss: 0.0784\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0792 - val_loss: 0.0783\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.0791 - val_loss: 0.0783\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0791 - val_loss: 0.0782\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0790 - val_loss: 0.0782\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.0790 - val_loss: 0.0783\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0789 - val_loss: 0.0782\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.0789 - val_loss: 0.0782\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0789 - val_loss: 0.0781\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0788 - val_loss: 0.0780\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0788 - val_loss: 0.0781\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0788 - val_loss: 0.0780\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.0787 - val_loss: 0.0781\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0787 - val_loss: 0.0780\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0787 - val_loss: 0.0779\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.0787 - val_loss: 0.0780\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0787 - val_loss: 0.0779\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0786 - val_loss: 0.0781\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.0786 - val_loss: 0.0779\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.0786 - val_loss: 0.0779\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0786 - val_loss: 0.0779\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.0786 - val_loss: 0.0779\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.0786 - val_loss: 0.0778\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.0786 - val_loss: 0.0779\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.0786 - val_loss: 0.0779\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.0786 - val_loss: 0.0778\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0786 - val_loss: 0.0779\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0785 - val_loss: 0.0779\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0785 - val_loss: 0.0779\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test  = x_test .astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "x_test  = np.reshape(x_test,  (len(x_test),  28, 28, 1))\n",
    "\n",
    "from keras.layers import Input, Dense, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def create_dense_ae():\n",
    "    # Размерность кодированного представления\n",
    "    encoding_dim = 49\n",
    "\n",
    "    # Энкодер\n",
    "    # Входной плейсхолдер\n",
    "    input_img = Input(\n",
    "        shape=(28, 28, 1))  # 28, 28, 1 - размерности строк, столбцов, фильтров одной картинки, без батч-размерности\n",
    "    # Вспомогательный слой решейпинга\n",
    "    flat_img = Flatten()(input_img)\n",
    "    # Кодированное полносвязным слоем представление\n",
    "    encoded = Dense(encoding_dim, activation='relu')(flat_img)\n",
    "\n",
    "    # Декодер\n",
    "    # Раскодированное другим полносвязным слоем изображение\n",
    "    input_encoded = Input(shape=(encoding_dim,))\n",
    "    flat_decoded = Dense(28 * 28, activation='sigmoid')(input_encoded)\n",
    "    decoded = Reshape((28, 28, 1))(flat_decoded)\n",
    "\n",
    "    # Модели, в конструктор первым аргументом передаются входные слои, а вторым выходные слои\n",
    "    # Другие модели можно так же использовать как и слои\n",
    "    encoder = Model(input_img, encoded, name=\"encoder\")\n",
    "    decoder = Model(input_encoded, decoded, name=\"decoder\")\n",
    "    autoencoder = Model(input_img, decoder(encoder(input_img)), name=\"autoencoder\")\n",
    "    return encoder, decoder, autoencoder\n",
    "\n",
    "\n",
    "encoder, decoder, autoencoder = create_dense_ae()\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_digits(*args):\n",
    "    args = [x.squeeze() for x in args]\n",
    "    n = min([x.shape[0] for x in args])\n",
    "\n",
    "    plt.figure(figsize=(2 * n, 2 * len(args)))\n",
    "    for j in range(n):\n",
    "        for i in range(len(args)):\n",
    "            ax = plt.subplot(len(args), n, i * n + j + 1)\n",
    "            plt.imshow(args[i][j])\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'y']\n"
     ]
    }
   ],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_csv.reader object at 0x7f763c0b6358>\n"
     ]
    }
   ],
   "source": [
    "with open('water_threatment_8x5000.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    print(readCSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     1 completed out of    100. Loss:  308.874. Accuracy: 1.000.\n",
      "Epoch     2 completed out of    100. Loss:  246.267. Accuracy: 1.000.\n",
      "Epoch     3 completed out of    100. Loss:  186.485. Accuracy: 1.000.\n",
      "Epoch     4 completed out of    100. Loss:  129.683. Accuracy: 1.000.\n",
      "Epoch     5 completed out of    100. Loss:   75.926. Accuracy: 0.999.\n",
      "Epoch     6 completed out of    100. Loss:   27.456. Accuracy: 0.555.\n",
      "Epoch     7 completed out of    100. Loss:   12.834. Accuracy: 0.635.\n",
      "Epoch     8 completed out of    100. Loss:   16.590. Accuracy: 0.927.\n",
      "Epoch     9 completed out of    100. Loss:   17.948. Accuracy: 0.814.\n",
      "Epoch    10 completed out of    100. Loss:   14.216. Accuracy: 0.723.\n",
      "Epoch    11 completed out of    100. Loss:   14.713. Accuracy: 0.823.\n",
      "Epoch    12 completed out of    100. Loss:   15.654. Accuracy: 0.804.\n",
      "Epoch    13 completed out of    100. Loss:   14.850. Accuracy: 0.781.\n",
      "Epoch    14 completed out of    100. Loss:   14.939. Accuracy: 0.804.\n",
      "Epoch    15 completed out of    100. Loss:   15.282. Accuracy: 0.800.\n",
      "Epoch    16 completed out of    100. Loss:   15.124. Accuracy: 0.794.\n",
      "Epoch    17 completed out of    100. Loss:   15.196. Accuracy: 0.801.\n",
      "Epoch    18 completed out of    100. Loss:   15.332. Accuracy: 0.801.\n",
      "Epoch    19 completed out of    100. Loss:   15.322. Accuracy: 0.799.\n",
      "Epoch    20 completed out of    100. Loss:   15.381. Accuracy: 0.800.\n",
      "Epoch    21 completed out of    100. Loss:   15.442. Accuracy: 0.799.\n",
      "Epoch    22 completed out of    100. Loss:   15.459. Accuracy: 0.798.\n",
      "Epoch    23 completed out of    100. Loss:   15.494. Accuracy: 0.796.\n",
      "Epoch    24 completed out of    100. Loss:   15.515. Accuracy: 0.795.\n",
      "Epoch    25 completed out of    100. Loss:   15.522. Accuracy: 0.796.\n",
      "Epoch    26 completed out of    100. Loss:   15.531. Accuracy: 0.799.\n",
      "Epoch    27 completed out of    100. Loss:   15.530. Accuracy: 0.798.\n",
      "Epoch    28 completed out of    100. Loss:   15.525. Accuracy: 0.800.\n",
      "Epoch    29 completed out of    100. Loss:   15.517. Accuracy: 0.799.\n",
      "Epoch    30 completed out of    100. Loss:   15.507. Accuracy: 0.799.\n",
      "Epoch    31 completed out of    100. Loss:   15.499. Accuracy: 0.796.\n",
      "Epoch    32 completed out of    100. Loss:   15.495. Accuracy: 0.796.\n",
      "Epoch    33 completed out of    100. Loss:   15.493. Accuracy: 0.797.\n",
      "Epoch    34 completed out of    100. Loss:   15.495. Accuracy: 0.797.\n",
      "Epoch    35 completed out of    100. Loss:   15.501. Accuracy: 0.794.\n",
      "Epoch    36 completed out of    100. Loss:   15.504. Accuracy: 0.792.\n",
      "Epoch    37 completed out of    100. Loss:   15.501. Accuracy: 0.789.\n",
      "Epoch    38 completed out of    100. Loss:   15.492. Accuracy: 0.786.\n",
      "Epoch    39 completed out of    100. Loss:   15.477. Accuracy: 0.785.\n",
      "Epoch    40 completed out of    100. Loss:   15.458. Accuracy: 0.782.\n",
      "Epoch    41 completed out of    100. Loss:   15.433. Accuracy: 0.780.\n",
      "Epoch    42 completed out of    100. Loss:   15.403. Accuracy: 0.779.\n",
      "Epoch    43 completed out of    100. Loss:   15.369. Accuracy: 0.779.\n",
      "Epoch    44 completed out of    100. Loss:   15.329. Accuracy: 0.777.\n",
      "Epoch    45 completed out of    100. Loss:   15.285. Accuracy: 0.777.\n",
      "Epoch    46 completed out of    100. Loss:   15.235. Accuracy: 0.775.\n",
      "Epoch    47 completed out of    100. Loss:   15.181. Accuracy: 0.772.\n",
      "Epoch    48 completed out of    100. Loss:   15.121. Accuracy: 0.769.\n",
      "Epoch    49 completed out of    100. Loss:   15.655. Accuracy: 0.797.\n",
      "Epoch    50 completed out of    100. Loss:   20.825. Accuracy: 0.791.\n",
      "Epoch    51 completed out of    100. Loss:   19.187. Accuracy: 0.791.\n",
      "Epoch    52 completed out of    100. Loss:   19.005. Accuracy: 0.810.\n",
      "Epoch    53 completed out of    100. Loss:   15.791. Accuracy: 0.771.\n",
      "Epoch    54 completed out of    100. Loss:   15.827. Accuracy: 0.782.\n",
      "Epoch    55 completed out of    100. Loss:   15.535. Accuracy: 0.776.\n",
      "Epoch    56 completed out of    100. Loss:   15.320. Accuracy: 0.766.\n",
      "Epoch    57 completed out of    100. Loss:   14.424. Accuracy: 0.775.\n",
      "Epoch    58 completed out of    100. Loss:   13.968. Accuracy: 0.770.\n",
      "Epoch    59 completed out of    100. Loss:   14.209. Accuracy: 0.771.\n",
      "Epoch    60 completed out of    100. Loss:   14.316. Accuracy: 0.773.\n",
      "Epoch    61 completed out of    100. Loss:   14.272. Accuracy: 0.776.\n",
      "Epoch    62 completed out of    100. Loss:   14.149. Accuracy: 0.774.\n",
      "Epoch    63 completed out of    100. Loss:   14.025. Accuracy: 0.778.\n",
      "Epoch    64 completed out of    100. Loss:   13.962. Accuracy: 0.776.\n",
      "Epoch    65 completed out of    100. Loss:   13.916. Accuracy: 0.779.\n",
      "Epoch    66 completed out of    100. Loss:   13.847. Accuracy: 0.777.\n",
      "Epoch    67 completed out of    100. Loss:   13.780. Accuracy: 0.776.\n",
      "Epoch    68 completed out of    100. Loss:   13.721. Accuracy: 0.774.\n",
      "Epoch    69 completed out of    100. Loss:   13.673. Accuracy: 0.772.\n",
      "Epoch    70 completed out of    100. Loss:   13.644. Accuracy: 0.773.\n",
      "Epoch    71 completed out of    100. Loss:   13.634. Accuracy: 0.770.\n",
      "Epoch    72 completed out of    100. Loss:   13.629. Accuracy: 0.772.\n",
      "Epoch    73 completed out of    100. Loss:   13.630. Accuracy: 0.769.\n",
      "Epoch    74 completed out of    100. Loss:   13.636. Accuracy: 0.767.\n",
      "Epoch    75 completed out of    100. Loss:   13.662. Accuracy: 0.766.\n",
      "Epoch    76 completed out of    100. Loss:   13.715. Accuracy: 0.765.\n",
      "Epoch    77 completed out of    100. Loss:   13.803. Accuracy: 0.764.\n",
      "Epoch    78 completed out of    100. Loss:   13.932. Accuracy: 0.761.\n",
      "Epoch    79 completed out of    100. Loss:   14.064. Accuracy: 0.760.\n",
      "Epoch    80 completed out of    100. Loss:   14.135. Accuracy: 0.756.\n",
      "Epoch    81 completed out of    100. Loss:   14.168. Accuracy: 0.750.\n",
      "Epoch    82 completed out of    100. Loss:   14.200. Accuracy: 0.746.\n",
      "Epoch    83 completed out of    100. Loss:   14.239. Accuracy: 0.737.\n",
      "Epoch    84 completed out of    100. Loss:   14.285. Accuracy: 0.731.\n",
      "Epoch    85 completed out of    100. Loss:   14.340. Accuracy: 0.721.\n",
      "Epoch    86 completed out of    100. Loss:   14.400. Accuracy: 0.714.\n",
      "Epoch    87 completed out of    100. Loss:   14.466. Accuracy: 0.700.\n",
      "Epoch    88 completed out of    100. Loss:   14.535. Accuracy: 0.691.\n",
      "Epoch    89 completed out of    100. Loss:   14.605. Accuracy: 0.683.\n",
      "Epoch    90 completed out of    100. Loss:   14.673. Accuracy: 0.675.\n",
      "Epoch    91 completed out of    100. Loss:   14.740. Accuracy: 0.665.\n",
      "Epoch    92 completed out of    100. Loss:   14.806. Accuracy: 0.658.\n",
      "Epoch    93 completed out of    100. Loss:   14.871. Accuracy: 0.653.\n",
      "Epoch    94 completed out of    100. Loss:   14.937. Accuracy: 0.646.\n",
      "Epoch    95 completed out of    100. Loss:   15.003. Accuracy: 0.638.\n",
      "Epoch    96 completed out of    100. Loss:   15.068. Accuracy: 0.631.\n",
      "Epoch    97 completed out of    100. Loss:   15.136. Accuracy: 0.627.\n",
      "Epoch    98 completed out of    100. Loss:   15.205. Accuracy: 0.626.\n",
      "Epoch    99 completed out of    100. Loss:   15.275. Accuracy: 0.624.\n",
      "Epoch   100 completed out of    100. Loss:   15.348. Accuracy: 0.623.\n"
     ]
    }
   ],
   "source": [
    "# Для загрузки данных из csv.\n",
    "import csv\n",
    "# Библиотеки машинного обучения.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# Библиотека для работы с массивами.\n",
    "import numpy as np\n",
    "# Библиотека для построения графиков.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Зерно для воспроизводимости.\n",
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "# TODO: Почему не работает? Исправить!\n",
    "tf.set_random_seed(random_state)\n",
    "tf.random.set_random_seed(random_state)\n",
    "\n",
    "\n",
    "# Функция создания обучающей и тестовой выборок.\n",
    "def create_feature_sets_and_labels(file, test_size = 0.2):\n",
    "    # Открытие csv-файла.\n",
    "    with open(file) as csvfile:\n",
    "        readCSV = np.genfromtxt(file, delimiter=',')\n",
    "        readCSV = np.delete(readCSV, 0, 0)\n",
    "        \n",
    "        # Строка для отладки.\n",
    "        # print(readCSV)\n",
    "        \n",
    "        np.random.shuffle(readCSV)\n",
    "        \n",
    "        # Строка для отладки.\n",
    "        # print(readCSV)\n",
    "        \n",
    "        # Строки для отладки.\n",
    "        '''\n",
    "        print(len(readCSV))\n",
    "        print(readCSV[0:2,0:2])\n",
    "        for row in readCSV:\n",
    "            print(row)\n",
    "        '''\n",
    "        \n",
    "        # Деление выборки\n",
    "        testing_size = int(test_size*len(readCSV))\n",
    "        train_x = readCSV[:-testing_size,:-1]\n",
    "        train_y = readCSV[:-testing_size,-1:]\n",
    "        test_x = readCSV[-testing_size:,:-1]\n",
    "        test_y = readCSV[-testing_size:,-1:]\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "\n",
    "# Создание обучающей и тестовой выборок из csv-файла с исходной выборкой.\n",
    "train_x, train_y, test_x, test_y = create_feature_sets_and_labels('water_threatment_8x5000.csv')\n",
    "\n",
    "# Строки для отладки.\n",
    "'''\n",
    "print()\n",
    "print(len(train_x))\n",
    "print(train_x)\n",
    "print()\n",
    "print(len(train_y))\n",
    "print(train_y)\n",
    "print()\n",
    "print(len(test_x))\n",
    "print(test_x)\n",
    "print()\n",
    "print(len(test_y))\n",
    "print(test_y)\n",
    "'''\n",
    "\n",
    "# Другой способ задания количества нейронов в слое.\n",
    "'''\n",
    "n_neurons = 13\n",
    "n_nodes_hl1 = n_neurons\n",
    "n_nodes_hl2 = n_neurons\n",
    "n_nodes_hl3 = n_neurons\n",
    "'''\n",
    "\n",
    "# Количество нейронов в слоях нейронной сети.\n",
    "n_nodes_hl1 = 11\n",
    "n_nodes_hl2 = 8\n",
    "n_nodes_hl3 = 5\n",
    "\n",
    "# Количество классов - два.\n",
    "n_classes = 2\n",
    "\n",
    "# При обучении нейронная сеть будет получать наблюдения пачками.\n",
    "# Это размер пачки.\n",
    "batch_size = 500\n",
    "\n",
    "# Переменные для обучающей и тестовой выборок,\n",
    "# которые будут инициализированы при обучении нейронной сети.\n",
    "x = tf.placeholder('float',[None, 8])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "\n",
    "# Задание структуры нейронной сети.\n",
    "# TODO: Выяснить почему не работает задание зерна для функций tensorflow.\n",
    "def neural_network_model(data):\n",
    "    # Задание весов нейронов для 3 скрытых слоев и выходного слоя.\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([8, n_nodes_hl1])),\n",
    "                     'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                     'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                     'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                   'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "    # Описание процесса обучения сети с использованием функции активации RELU.\n",
    "    # TODO: Попробовать другие функции обучения после того, как работа нейронной сети\n",
    "    # станет стабильной.\n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    output = tf.matmul(l3, output_layer['weights']) + output_layer['biases']\n",
    "    # Выдача функцией выходных значений.\n",
    "    return output\n",
    "\n",
    "\n",
    "# Функция обучения нейронной сети.\n",
    "def train_neural_network(x):\n",
    "    # Получение прогноза нейронной сетью.\n",
    "    prediction = neural_network_model(x)\n",
    "    # Вычисление функции потерь.\n",
    "    # TODO: 1. Подобрать функцию с перекрестной энтропией, более подходящую для\n",
    "    # бинарной классификации, если такая найдется.\n",
    "    # TODO: 2. Переделать под совместимость со следующим обновлением библиотеки.\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y))\n",
    "    # Минимизация функции потерь.\n",
    "    # TODO: Посмотреть какие еще есть функции минимизации.\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    # Количество эпох обучения.\n",
    "    # TODO: Вынести из функции обучения сети.\n",
    "    hm_epochs = 100\n",
    "    # Процесс обучения нейронной сети - работает только тогда,\n",
    "    # когда вызывается функция обучения.\n",
    "    with tf.Session() as sess:\n",
    "        # Инициализация переменных.\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        # Процесс обучения производится заданное количество эпох.\n",
    "        for epoch in range(hm_epochs):\n",
    "            # Переменная потерь за эпоху.\n",
    "            epoch_loss = 0\n",
    "            # Цикл обучения по пачкам, на случай очень большого количества данных.\n",
    "            i = 0\n",
    "            while i < len(train_x):\n",
    "                start = i\n",
    "                end = i + batch_size\n",
    "                # Пачки наблюдений.\n",
    "                batch_x = np.array(train_x[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "                # Расчет и оптимизация функции потерь.\n",
    "                _, c = sess.run([optimizer, loss], feed_dict={x: batch_x,y: batch_y})\n",
    "                # Подсчет потерь.\n",
    "                epoch_loss += c\n",
    "                # Переход к следующей пачке наблюдений.\n",
    "                i += batch_size\n",
    "            \n",
    "            # Вариант подачи выборки на обучение нейронной сети.\n",
    "                '''\n",
    "                for _ in range(int(water_threatment/batch_size)):\n",
    "                    x, y = \n",
    "                '''\n",
    "            \n",
    "            # Количество правильных прогнозов.\n",
    "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "            # Доля правильных прогнозов.\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "            # Информация об обучении за эпоху.\n",
    "            print('Epoch {:5.0f}'.format(epoch + 1),\n",
    "                  'completed out of {:6.0f}.'.format(hm_epochs),\n",
    "                  'Loss: {:8.3f}.'.format(epoch_loss),\n",
    "                  'Accuracy: {:1.3f}.'.format(accuracy.eval({x:test_x, y:test_y})))\n",
    "\n",
    "\n",
    "# Обучение нейронной сети.\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
